{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "from utils import EarlyStopping\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: multi_woz_v22/v2.2_active_only\n",
      "Found cached dataset multi_woz_v22 (/data/.cache/huggingface/datasets/multi_woz_v22/v2.2_active_only/2.2.0/6719c8b21478299411a0c6fdb7137c3ebab2e6425129af831687fb7851c69eb5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75cd7413be74dd2a907128cf5fab5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = datasets.load_dataset(path='multi_woz_v22', cache_dir='/data/.cache/huggingface/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.4.attn.masked_bias', 'h.10.attn.masked_bias', 'classifier.weight', 'h.0.attn.masked_bias', 'h.11.attn.masked_bias', 'h.2.attn.masked_bias', 'h.9.attn.masked_bias', 'h.7.attn.masked_bias', 'h.3.attn.masked_bias', 'h.1.attn.masked_bias', 'h.6.attn.masked_bias', 'h.8.attn.masked_bias', 'classifier.bias', 'h.5.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', cache_dir='/data/.cache/huggingface/transformers', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>', return_value='pt')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.AutoModelForTokenClassification.from_pretrained('gpt2', cache_dir='/data/.cache/huggingface/transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer = tokenizer, max_length=None):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = self.tokenizer.model_max_length if max_length is None else max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        output = {}\n",
    "        utterances = self.data[index]['turns']['utterance']\n",
    "        each_utt = [re.sub(\"[^ +a-zA-Z0-9]+\", \"\", x) for x in utterances]\n",
    "        each_utt = [x.lower() for x in each_utt]\n",
    "        label = tokenizer(each_utt)['attention_mask']\n",
    "        for i in range(len(label)):\n",
    "            label[i][-1] = 0\n",
    "        token_label = torch.tensor(sum(label, []), dtype=torch.long)[:self.max_length]\n",
    "        token_label = (~token_label.bool()).float()\n",
    "        if len(token_label) < self.max_length: # assign padding token label\n",
    "            token_label = torch.cat([token_label[:self.max_length], torch.zeros(self.max_length - len(token_label))])\n",
    "        token_dict = tokenizer(' '.join(each_utt), truncation=True, max_length=self.max_length, padding=\"max_length\", return_tensors='pt')\n",
    "        output['input_ids'], output['attention_mask'] = token_dict['input_ids'], token_dict['attention_mask']\n",
    "        output['labels'] = token_label.type(torch.LongTensor)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1024]), torch.Size([1, 1024]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########TODO: shape 해결 필요 , squeeze() 하지 않게.\n",
    "train_dataset[0]['input_ids'].shape, train_dataset[0]['attention_mask'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DialogDataset(data['train'], tokenizer)\n",
    "valid_dataset = DialogDataset(data['validation'], tokenizer)\n",
    "test_dataset = DialogDataset(data['test'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "early_stopping = EarlyStopping(patience=3, verbose=True, path=f'saved/best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f5676525c34161a062be2cdf1d9419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c97f0504f474b008054257266f14d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1925, Valid Loss: 0.1721\n",
      "\n",
      "Validation loss decreased (inf --> 10.841613).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4383bc0cf73f4941b90d0d9285d3aa44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc7515a00464c7387df7aa1a4e6bd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.3549, Valid Loss: 0.1635\n",
      "\n",
      "Validation loss decreased (10.841613 --> 10.299261).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189f4cc69a9046d28f5581e0ba8948ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ee5091630645b2a91283b0c78d1d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.5050, Valid Loss: 0.1616\n",
      "\n",
      "Validation loss decreased (10.299261 --> 10.179914).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cbcfd3c91b4424a2f48f0b44a2b3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb11a22111b407aa3901a8a46c1682e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.6434, Valid Loss: 0.1668\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615aefa8ec384bab904ffadd4c1e7af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349f1426f00043e6a59a565bd0100add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.7695, Valid Loss: 0.1629\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a5473adc0c4407b129835411abbc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c698a5e555a14a8cb155650e542e0f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.8832, Valid Loss: 0.1773\n",
      "EarlyStopping counter: 3 out of 3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dongi/segmentation/segmentation.ipynb 셀 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244534c61625f54696e79227d/home/dongi/segmentation/segmentation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m early_stopping(valid_loss, model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244534c61625f54696e79227d/home/dongi/segmentation/segmentation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m early_stopping\u001b[39m.\u001b[39mearly_stop:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244534c61625f54696e79227d/home/dongi/segmentation/segmentation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEarly stopping at \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m-\u001b[39mearly_stopping_patience\u001b[39m}\u001b[39;00m\u001b[39m !\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244534c61625f54696e79227d/home/dongi/segmentation/segmentation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "train_loss = 0.0\n",
    "for each_epoch in range(1, epochs):\n",
    "    model.train()\n",
    "    for each_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = each_batch['input_ids'].to(device)\n",
    "        attention_mask = each_batch['attention_mask'].to(device)\n",
    "        labels = each_batch['labels'].to(device)\n",
    "        out = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        for each_batch in tqdm(valid_loader):\n",
    "            input_ids = each_batch['input_ids'].to(device)\n",
    "            attention_mask = each_batch['attention_mask'].to(device)\n",
    "            labels = each_batch['labels'].to(device)\n",
    "            out = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            valid_loss += out.loss.item()\n",
    "        print(f'Epoch {each_epoch}: Train Loss: {train_loss / len(train_loader):.4f}, Valid Loss: {valid_loss / len(valid_loader):.4f}')\n",
    "\n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at {each_epoch-early_stopping.patience} !\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 1024, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "\tmodel.load_state_dict(torch.load(f'saved/best_model.pt'))\n",
    "\tmodel.eval()\n",
    "\tlogit_test = []\n",
    "\tpred_test = []\n",
    "\treal_test = []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor each_batch in tqdm(test_loader):\n",
    "\t\t\tinput_ids = each_batch['input_ids'].to(device)\n",
    "\t\t\tattention_mask = each_batch['attention_mask'].to(device)\n",
    "\t\t\tout = model(input_ids, attention_mask=attention_mask)\n",
    "\t\t\tlogit_test.append(out.logits.cpu())\n",
    "\t\t\tpred_test.append(torch.argmax(out.logits, dim=-1).cpu())\n",
    "\t\t\treal_test.append(each_batch['labels'].cpu())\n",
    "\treturn torch.cat(logit_test), torch.cat(pred_test), torch.cat(real_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(pred):\n",
    "\tprediction = []\n",
    "\tfor each_pred in pred:\n",
    "\t\tprediction.append(each_pred[0])\n",
    "\treturn prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c86e6f4c55245f2b3f02bce379b8854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_logit, test_pred, test_real = test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 1, 1024, 2]),\n",
       " torch.Size([1000, 1, 1024]),\n",
       " torch.Size([1000, 1024]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_logit.shape, test_pred.shape, test_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3055)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall w.r.t. class==1 (끝 지점)\n",
    "test_recall = ((test_pred.squeeze()==1) & (test_real==1)).sum() / (test_real==1).sum()\n",
    "test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4907)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision w.r.t. class==1 (끝 지점)\n",
    "test_precision = ((test_pred.squeeze()==1) & (test_real==1)).sum() / (test_pred.squeeze()==1).sum()\n",
    "test_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3765)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f1 w.r.t. class==1 (끝 지점)\n",
    "test_f1_turn = 2 * test_precision * test_recall / (test_precision + test_recall)\n",
    "test_f1_turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sample(sample, sample_logits):\n",
    "    end_count = 0\n",
    "    for each_sample_id, each_sample_logit in zip(sample['input_ids'].squeeze(), sample_logits.squeeze()):\n",
    "        print(tokenizer.decode(each_sample_id), end= \" \")\n",
    "        if each_sample_logit.argmax() ==1:   # threshold 없이 argmax가 1인 지점\n",
    "            print()\n",
    "            print(f'        %Prob% : {torch.sigmoid(each_sample_logit[1]):.4f}')\n",
    "            print(f'        %Logit% : {each_sample_logit[1].item()}')\n",
    "        if each_sample_id==50256:\n",
    "            end_count+=1\n",
    "        else:\n",
    "            end_count=0\n",
    "        if end_count>5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch 3 (=early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  need  train  reservations  from  nor wich  to  cam bridge \n",
      "        %Prob% : 0.4344\n",
      "        %Logit% : -0.26409125328063965\n",
      " i  have  133  trains  matching  your  request  is  there  a  specific  day  and  time  you  would  like  to  travel \n",
      "        %Prob% : 0.5647\n",
      "        %Logit% : 0.26006728410720825\n",
      " id  like  to  leave  on  m onday  and  arrive  by  1800  there  are  12  trains  for  the  day  and  time  you  request  would  you  like  to  book  it  now \n",
      "        %Prob% : 0.5528\n",
      "        %Logit% : 0.21185562014579773\n",
      " before  booking  i  would  also  like  to  know  the  travel  time  price  and  departure  time  please \n",
      "        %Prob% : 0.6617\n",
      "        %Logit% : 0.6708519458770752\n",
      " there  are  12  trains  meeting  your  needs  with  the  first  leaving  at  05 16  and  the  last  one  leaving  at  16 16  do  you  want  to  book  one  of  these  no  hold  off  on  booking  for  now  can  you  help  me  find  an  attraction  called  c in eworld  cinema \n",
      "        %Prob% : 0.5407\n",
      "        %Logit% : 0.16315631568431854\n",
      " yes  it  is  a  cinema  located  in  the  south  part  of  town  what  information  would  you  like  on  it  yes  that  was  all  i  needed  thank  you  very  much  thank  you  for  using  our  system <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> "
     ]
    }
   ],
   "source": [
    "extract_sample(test_dataset[0], test_logit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I need train reservations from norwich to cambridge',\n",
       " 'I have 133 trains matching your request. Is there a specific day and time you would like to travel?',\n",
       " \"I'd like to leave on Monday and arrive by 18:00.\",\n",
       " 'There are 12 trains for the day and time you request. Would you like to book it now?',\n",
       " 'Before booking, I would also like to know the travel time, price, and departure time please.',\n",
       " 'There are 12 trains meeting your needs with the first leaving at 05:16 and the last one leaving at 16:16. Do you want to book one of these?',\n",
       " 'No hold off on booking for now. Can you help me find an attraction called cineworld cinema?',\n",
       " 'Yes it is a cinema located in the south part of town what information would you like on it?',\n",
       " 'Yes, that was all I needed. Thank you very much!',\n",
       " 'Thank you for using our system.']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['test'][0]['turns']['utterance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello  i  am  looking  for  a  restaurant  in  cam bridge  i  believe  it  is  called  golden  w ok  it  is  located  at  191  hist on  road  chest erton  can  you  book  me  a  table  for  1100  on  fr iday  yes  i  can  table  for  1  actually  for  4  please  okay  your  booking  was  successful  the  reference  number  is  m uf c my ff    the  table  will  be  reserved  for  15  minutes  great \n",
      "        %Prob% : 0.5345\n",
      "        %Logit% : 0.13834477961063385\n",
      " can  you  also  get  me  information  or  architecture  in  the  area  sure  there  are  several  churches  and  an  old  schools  attraction  all  in  the  centre  area  do  you  have  a  preference  what  do  you  recommend  old  schools  is  lovely  they  are  on  tr inity  lane  and  free  admission  can  i  get  the  post code  for  that  i  also  need  to  book  a  taxi  to  the  golden  w ok  the  post code  is  c b 21 tt  are  you  looking  for  a  taxi  from  old  schools  to  the  golden  w ok  yes  i  do  id  like  to  make  sure  i  arrive  at  the  restaurant  by  the  booked  time  can  you  check  what  time  do  you  want  to  leave  actually  all  you  have  to  do  is  set  the  taxi  so  it  arrives  by  the  arrived  time  am  i  better  off  booking  it  myself  i  have  booked  you  a  taxi  for  fr iday  to  arrive  at  old  schools  at  10 45  is  there  anything  else  i  may  help  you  with  i  need  the  contact  number  and  car  type  also  it  is  a  lex us  white  in  colour  and  contact  number  is  07 38 18 84 388  thanks  for  the  service  good  day  you re  welcome  have  a  great  day <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> "
     ]
    }
   ],
   "source": [
    "extract_sample(test_dataset[1], test_logit[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, I am looking for a restaurant in Cambridge. I believe it is called Golden Wok.',\n",
       " 'It is located at 191 Histon Road Chesterton',\n",
       " 'Can you book me a table for 11:00 on Friday?',\n",
       " 'Yes I can! Table for 1?',\n",
       " 'Actually, for 4, please.',\n",
       " 'Okay, your booking was successful! The reference number is MUFCMYFF . The table will be reserved for 15 minutes.',\n",
       " 'Great, can you also get me information or architecture in the area',\n",
       " 'Sure. There are several churches and an old schools attraction, all in the centre area. Do you have a preference?',\n",
       " 'What do you recommend?',\n",
       " 'old schools is lovely, they are on trinity lane and free admission',\n",
       " 'Can I get the postcode for that? I also need to book a taxi to the Golden Wok.',\n",
       " 'The postcode is cb21tt. Are you looking for a taxi from Old Schools to the Golden Wok?',\n",
       " \"Yes I do. I'd like to make sure I arrive at the restaurant by the booked time. Can you check?\",\n",
       " 'What time do you want to leave?',\n",
       " 'Actually all you have to do is set the taxi so it arrives by the arrived time. Am I better off booking it myself?',\n",
       " 'I have booked you a taxi for Friday to arrive at Old Schools at 10:45. Is there anything else I may help you with?',\n",
       " 'I need the contact number and car type also.',\n",
       " 'it is a Lexus white in colour and contact number is 07381884388',\n",
       " 'Thanks for the service, good day.',\n",
       " \"You're welcome! Have a great day!\"]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['test'][1]['turns']['utterance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch 6 (= final training) version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3969)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recall = ((test_pred.squeeze()==1) & (test_real==1)).sum() / (test_real==1).sum()\n",
    "test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1838)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_precision = ((test_pred.squeeze()==1) & (test_real==1)).sum() / (test_pred.squeeze()==1).sum()\n",
    "test_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2512)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1_turn = 2 * test_precision * test_recall / (test_precision + test_recall)\n",
    "test_f1_turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  need  train  reservations  from  nor wich  to  cam bridge  i  have  133  trains  matching  your  request  is  there  a  specific  day  and  time  you  would  like  to  travel \n",
      "        %Prob% : 0.7557\n",
      "        %Logit% : 1.1293197870254517\n",
      " id  like  to  leave  on  m onday  and  arrive  by  1800 \n",
      "        %Prob% : 0.4216\n",
      "        %Logit% : -0.316301167011261\n",
      " there  are  12  trains  for  the  day  and  time  you  request \n",
      "        %Prob% : 0.4253\n",
      "        %Logit% : -0.30112752318382263\n",
      " would  you  like  to  book  it  now \n",
      "        %Prob% : 0.5885\n",
      "        %Logit% : 0.3577260971069336\n",
      " before  booking  i  would  also  like  to  know  the  travel  time  price \n",
      "        %Prob% : 0.5151\n",
      "        %Logit% : 0.060555506497621536\n",
      " and  departure \n",
      "        %Prob% : 0.4736\n",
      "        %Logit% : -0.10589227080345154\n",
      " time  please \n",
      "        %Prob% : 0.6415\n",
      "        %Logit% : 0.5819669365882874\n",
      " there  are  12  trains  meeting  your  needs  with  the  first  leaving  at  05 16  and  the  last  one  leaving  at  16 16  do  you  want  to  book  one  of  these  no  hold  off  on  booking  for  now  can  you  help  me  find  an  attraction  called  c in eworld  cinema \n",
      "        %Prob% : 0.7366\n",
      "        %Logit% : 1.0284615755081177\n",
      " yes  it  is  a  cinema \n",
      "        %Prob% : 0.6454\n",
      "        %Logit% : 0.5990073084831238\n",
      " located  in  the  south  part  of  town  what  information  would  you  like  on  it  yes  that  was  all  i  needed  thank  you  very  much  thank  you  for  using  our  system <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> "
     ]
    }
   ],
   "source": [
    "extract_sample(test_dataset[0], test_logit[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/v3.0.2/_modules/transformers/modeling_bert.html#BertForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoModelForTokenClassification: \n",
    "1. sequence_output = AutoModel 의 output(각 토큰의 vector)\n",
    "2. sequence_output = dropout(config.hidden_dropout_prob)\n",
    "3. logits = Linear(token_dim -> num_labels)\n",
    "4. loss = CrossEntropy 인데, attention_mask가 1인 토큰만 계산. logits, labels 모두"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm",
   "language": "python",
   "name": "esm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
